# Kaggle-scikit-learn
Kaggle-scikit-learn


-------

## scikit-learn: Machine Learning in Python
https://scikit-learn.org/stable/

## 1. Supervised learning

### 1.1. Linear Models

      1.1.1. Ordinary Least Squares
      1.1.2. Ridge regression and classification
      1.1.3. Lasso
      1.1.4. Multi-task Lasso
      1.1.5. Elastic-Net
      1.1.6. Multi-task Elastic-Net
      1.1.7. Least Angle Regression
      1.1.8. LARS Lasso
      1.1.9. Orthogonal Matching Pursuit (OMP)
      1.1.10. Bayesian Regression
      1.1.11. Logistic regression
      1.1.12. Generalized Linear Regression
      1.1.13. Stochastic Gradient Descent - SGD
      1.1.14. Perceptron
      1.1.15. Passive Aggressive Algorithms
      1.1.16. Robustness regression: outliers and modeling errors
      1.1.17. Polynomial regression: extending linear models with basis functions

### 1.2. Linear and Quadratic Discriminant Analysis

      1.2.1. Dimensionality reduction using Linear Discriminant Analysis
      1.2.2. Mathematical formulation of the LDA and QDA classifiers
      1.2.3. Mathematical formulation of LDA dimensionality reduction
      1.2.4. Shrinkage and Covariance Estimator
      1.2.5. Estimation algorithms

### 1.3. Kernel ridge regression

### 1.4. Support Vector Machines

      1.4.1. Classification
      1.4.2. Regression
      1.4.3. Density estimation, novelty detection
      1.4.4. Complexity
      1.4.5. Tips on Practical Use
      1.4.6. Kernel functions
      1.4.7. Mathematical formulation
      1.4.8. Implementation details

### 1.5. Stochastic Gradient Descent

      1.5.1. Classification
      1.5.2. Regression
      1.5.3. Stochastic Gradient Descent for sparse data
      1.5.4. Complexity
      1.5.5. Stopping criterion
      1.5.6. Tips on Practical Use
      1.5.7. Mathematical formulation
      1.5.8. Implementation details

### 1.6. Nearest Neighbors

      1.6.1. Unsupervised Nearest Neighbors
      1.6.2. Nearest Neighbors Classification
      1.6.3. Nearest Neighbors Regression
      1.6.4. Nearest Neighbor Algorithms
      1.6.5. Nearest Centroid Classifier
      1.6.6. Nearest Neighbors Transformer
      1.6.7. Neighborhood Components Analysis

### 1.7. Gaussian Processes

      1.7.1. Gaussian Process Regression (GPR)
      1.7.2. GPR examples
      1.7.3. Gaussian Process Classification (GPC)
      1.7.4. GPC examples
      1.7.5. Kernels for Gaussian Processes

### 1.8. Cross decomposition

      1.8.1. PLSCanonical
      1.8.2. PLSSVD
      1.8.3. PLSRegression
      1.8.4. Canonical Correlation Analysis

### 1.9. Naive Bayes

      1.9.1. Gaussian Naive Bayes
      1.9.2. Multinomial Naive Bayes
      1.9.3. Complement Naive Bayes
      1.9.4. Bernoulli Naive Bayes
      1.9.5. Categorical Naive Bayes
      1.9.6. Out-of-core naive Bayes model fitting

### 1.10. Decision Trees

      1.10.1. Classification
      1.10.2. Regression
      1.10.3. Multi-output problems
      1.10.4. Complexity
      1.10.5. Tips on practical use
      1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
      1.10.7. Mathematical formulation
      1.10.8. Minimal Cost-Complexity Pruning

### 1.11. Ensemble methods

      1.11.1. Bagging meta-estimator
      1.11.2. Forests of randomized trees
      1.11.3. AdaBoost
      1.11.4. Gradient Tree Boosting
      1.11.5. Histogram-Based Gradient Boosting
      1.11.6. Voting Classifier
      1.11.7. Voting Regressor
      1.11.8. Stacked generalization

### 1.12. Multiclass and multioutput algorithms

      1.12.1. Multiclass classification
      1.12.2. Multilabel classification
      1.12.3. Multiclass-multioutput classification
      1.12.4. Multioutput regression

### 1.13. Feature selection

      1.13.1. Removing features with low variance
      1.13.2. Univariate feature selection
      1.13.3. Recursive feature elimination
      1.13.4. Feature selection using SelectFromModel
      1.13.5. Sequential Feature Selection
      1.13.6. Feature selection as part of a pipeline

### 1.14. Semi-supervised learning

      1.14.1. Self Training
      1.14.2. Label Propagation

### 1.15. Isotonic regression

### 1.16. Probability calibration

      1.16.1. Calibration curves
      1.16.2. Calibrating a classifier
      1.16.3. Usage

### 1.17. Neural network models (supervised)

      1.17.1. Multi-layer Perceptron
      1.17.2. Classification
      1.17.3. Regression
      1.17.4. Regularization
      1.17.5. Algorithms
      1.17.6. Complexity
      1.17.7. Mathematical formulation
      1.17.8. Tips on Practical Use
      1.17.9. More control with warm_start

-------

## Support Vector Machines
### 1.4. Support Vector Machines
https://scikit-learn.org/stable/modules/svm.html

      1.4.1. Classification
      1.4.2. Regression
      1.4.3. Density estimation, novelty detection
      1.4.4. Complexity
      1.4.5. Tips on Practical Use
      1.4.6. Kernel functions
      1.4.7. Mathematical formulation
      1.4.8. Implementation details

-------

